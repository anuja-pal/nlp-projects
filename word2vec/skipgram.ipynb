{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id          0\n",
       "source_id       80880\n",
       "source_name         0\n",
       "author           8219\n",
       "title              40\n",
       "description       383\n",
       "url                 0\n",
       "url_to_image     5624\n",
       "published_at        0\n",
       "content             0\n",
       "category           42\n",
       "full_content    46943\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking NA counts across columns\n",
    "news_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105375"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of rows\n",
    "len(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retaining only those rows where full_content of the news is available\n",
    "news_data_sub = news_data.dropna(subset=['full_content'])\n",
    "# retaining only the news article column for our word2vec purpose\n",
    "news_data_sub = news_data_sub[['full_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying again that there are no na values in our full_content column in news_data_sub\n",
    "news_data_sub.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing only sample_size rows to reduce processing time\n",
    "random.seed(1)\n",
    "sample_size = 100\n",
    "news_data_sub = news_data_sub.iloc[random.sample(range(len(news_data_sub)), sample_size)]\n",
    "news_data_sub.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In closing arguments for Sam Bankman-Fried's c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perry Carpenter is Chief Evangelist for KnowBe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you buy through our links, Insider may ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW DELHI: The BJP has lodged a complaint with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wittenberg Investment Management Inc. lowered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Cowen AND Company LLC raised its position in s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Dublin, Nov.  20, 2023  (GLOBE NEWSWIRE) -- Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>THE HAGUE, Netherlands --Anti-Islam populist G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>An elusive echidna feared extinct after disapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>\"The world isn't doing terribly well in averti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         full_content\n",
       "0   In closing arguments for Sam Bankman-Fried's c...\n",
       "1   Perry Carpenter is Chief Evangelist for KnowBe...\n",
       "2   When you buy through our links, Insider may ea...\n",
       "3   NEW DELHI: The BJP has lodged a complaint with...\n",
       "4   Wittenberg Investment Management Inc. lowered ...\n",
       "..                                                ...\n",
       "95  Cowen AND Company LLC raised its position in s...\n",
       "96  Dublin, Nov.  20, 2023  (GLOBE NEWSWIRE) -- Th...\n",
       "97  THE HAGUE, Netherlands --Anti-Islam populist G...\n",
       "98  An elusive echidna feared extinct after disapp...\n",
       "99  \"The world isn't doing terribly well in averti...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100 articles available to process!\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(news_data_sub)} articles available to process!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(doc: str) -> list:\n",
    "    '''\n",
    "    INPUT:\n",
    "        doc: in our use case, it will be a string of multiple sentences\n",
    "    OUTPUT:\n",
    "        processed_doc: a list of tokens for the input doc\n",
    "    '''\n",
    "    # convert to lower case\n",
    "    doc = doc.lower()\n",
    "\n",
    "    # remove urls, punctuations, special characters, numbers\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    doc = re.sub(r\"_+\", \"\", doc)\n",
    "    doc = url_pattern.sub('', doc)\n",
    "    doc = re.sub(r'[^\\w\\s]','', doc) \n",
    "    doc = re.sub(r'\\d+', '', doc)\n",
    "\n",
    "    # tokenize\n",
    "    doc = doc.split()\n",
    "\n",
    "    # remove stopwords\n",
    "    doc = list(filter(lambda x: x not in stopwords.words('english'), doc))\n",
    "\n",
    "    # stemming\n",
    "    stemmer = SnowballStemmer(\"english\", True)\n",
    "    doc = [stemmer.stem(word) for word in doc]\n",
    "\n",
    "    # lemmatisation\n",
    "    wnl = WordNetLemmatizer()\n",
    "    doc = list(map(lambda word: wnl.lemmatize(word, pos=\"v\"),doc))\n",
    "    \n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process_data(' https://www.google.com ______ 12345 @#$% The quick brown foxes are jumping over the lazy dogs!!!!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing news articles: 100%|██████████| 100/100 [00:03<00:00, 26.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# pre-processing every news article\n",
    "\n",
    "# we will store the pre-process article in a new column 'preprocessed_full_content'\n",
    "\n",
    "tqdm.pandas(desc=\"Pre-processing news articles\") \n",
    "news_data_sub['preprocessed_full_content'] = news_data_sub['full_content'].progress_apply(pre_process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Perry Carpenter is Chief Evangelist for KnowBe4 Inc., provider of the popular Security Awareness Training & Simulated Phishing platform.   Numerous studies confirm that the absence of security technology isn’t what tends to get organizations into trouble. On the contrary, it’s humans. People are the ones who make poor security decisions and judgment errors: They click on bogus links, visit the wrong websites, download malware-loaded files, take security for granted and use weak passwords. Knowingly or unknowingly, they can put organizations at risk. While a robust security culture has been hailed as an answer to most human-related security challenges, it continues to elude many businesses because it requires chief information security officers (CISCOs) to build relationships at various levels and understand the idiosyncrasies of various business units. Various reports highlight how many CISOs struggle with competing priorities and how their security strategies often lack alignment with business objectives. Who Is A BISO And How Do They Fit In? For CISOs to succeed in their role, they need to be cognizant of all units across the business to avoid unexpected issues or unresolved matters. They should know their audience and tailor their approach to their specific needs and objectives. By aligning their security programs with the overall business strategy, CISOs can effectively meet the requirements of different departments. CISOs must also effectively communicate the security challenges facing the organization. The idea is to foster responsible participation for deeper collaboration on security initiatives. Unfortunately, a majority of CISOs are spending their limited time firefighting issues rather than contributing to business strategy or forging relationships. This is where a business information security officer (BISO) can come in. According to Forrester, the BISO operates on behalf of the CISO, serving as an advisor and bridge to functional leaders. In other words, it’s a security role that puts business first. CISO Versus BISO A BISO usually works for the CISO either directly or via a dotted-line relationship. While the CISO manages the most senior strategic relationships (such as the C-suite and the board), the BISO typically partners with the senior leaders of the other business units. So a BISO is kind of like a mini-CISO for every division or the lines of business that they support. In large, multinational corporations, multiple BISOs representing different business groups can all roll up to a CISO. Not only does this help divide responsibility and improve the implementation of security programs but it also helps CISOs gain a better pulse of the business and the different security use cases and requirements. How Do BISOs Influence Security Culture? Security culture can be defined as the values, attitudes, customs, beliefs, and social behaviors that influence the security posture of an organization. It’s the stuff that drives secure behavior in employees (even when no one’s watching); it’s the security instinct that kicks in when someone sees something unusual or suspicious. Traditionally, most CISOs are not in close contact or communication with employees, and therefore, it is difficult for them to influence and promote a positive security culture. With the BISO role, it's different; since the BISO enjoys closer ties with various business groups and has a better understanding of employee requirements and sentiments, they are better positioned to influence culture change. Let’s look at different ways you can use a BISO to help strengthen security culture: When business models, products and services are being strategized or developed, security is often treated as an afterthought. You can use BISOs and their partnerships with other department leaders to help make sure security is present right from the start and woven across products, processes and each and every customer interaction. BISOs should have a good understanding of security risks, scenarios and employee behaviors within each department they serve. Use this understanding to have them develop training programs that are tailored to individuals, making the programs more pertinent and relatable. I've found that this personalization can boost engagement, ultimately improving the retention of the training. Since BISOs work closely with specific business groups, they should be able to explain security in a language employees can understand. The result is that employees can stay updated about security policies and procedures, potential risks and best practices, gaining a clearer picture of their own responsibilities towards security. A BISO serves as the point of contact for leaders to communicate security expectations, challenges and areas where security can contribute value to the business. This helps foster trust, confidence and collaboration among teams. Security culture is a top priority of most CISOs. That being said, they must also accept the reality that it's impossible for them to be everywhere. BISOs, on the other hand, can act as catalysts, influencers and change agents on behalf of CISOs, helping them build and nurture a resilient workforce. Forbes Business Council is the foremost growth and networking organization for business owners and leaders. Do I qualify?\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of an unprocessed article\n",
    "news_data_sub['full_content'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['perri',\n",
       " 'carpent',\n",
       " 'chief',\n",
       " 'evangelist',\n",
       " 'knowb',\n",
       " 'inc',\n",
       " 'provid',\n",
       " 'popular',\n",
       " 'secur',\n",
       " 'awar']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the same article but after preprocessing (first 10 tokens)\n",
    "news_data_sub['preprocessed_full_content'][1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our vocabulary\n",
    "vocab = sorted(list(set([word for pre_processed_article_list in news_data_sub.preprocessed_full_content for word in pre_processed_article_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6381"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandon',\n",
       " 'abba',\n",
       " 'abbi',\n",
       " 'abc',\n",
       " 'abdel',\n",
       " 'abduct',\n",
       " 'abdullah',\n",
       " 'abdulrahman',\n",
       " 'abdurahman',\n",
       " 'aberdeen']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10] #need to pre-process better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating (target, context) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_context_pairs(doc: list, window_size: int) -> list:\n",
    "    '''\n",
    "    INPUT: \n",
    "        doc: a list of pre-processed tokens of a news article\n",
    "        window_size: an integer specifying the window size around the centre/context word to create the context-target pairs\n",
    "    OUTPUT:\n",
    "        target_context_pairs: a list of the form [(target1, context1), (target1, context2),..], etc.\n",
    "        The number of targets for a given context depends on the window_size.\n",
    "        The example above is for window_size=1.\n",
    "    '''\n",
    "    target_context_pairs = []\n",
    "    for index in range(window_size,len(doc)-window_size-1):\n",
    "\n",
    "        target = doc[index]\n",
    "        context = doc[index-window_size:index+window_size+1]\n",
    "        context.remove(target)\n",
    "        target_context_pairs = target_context_pairs + [(target, word) for word in context]\n",
    "    return target_context_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating (target, context) pairs for each news article: 100%|██████████| 100/100 [00:01<00:00, 55.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating the target, context pairs for every news article\n",
    "\n",
    "# we will store the context target pairs in a new column 'target_context_pairs'\n",
    "\n",
    "tqdm.pandas(desc=\"Creating (target, context) pairs for each news article\") \n",
    "news_data_sub['target_context_pairs'] = news_data_sub['preprocessed_full_content'].progress_apply(lambda x: target_context_pairs(x, window_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_content</th>\n",
       "      <th>preprocessed_full_content</th>\n",
       "      <th>target_context_pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In closing arguments for Sam Bankman-Fried's c...</td>\n",
       "      <td>[close, argument, sam, bankmanfri, crimin, tri...</td>\n",
       "      <td>[(bankmanfri, close), (bankmanfri, argument), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perry Carpenter is Chief Evangelist for KnowBe...</td>\n",
       "      <td>[perri, carpent, chief, evangelist, knowb, inc...</td>\n",
       "      <td>[(evangelist, perri), (evangelist, carpent), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you buy through our links, Insider may ea...</td>\n",
       "      <td>[buy, link, insid, may, earn, affili, commissi...</td>\n",
       "      <td>[(may, buy), (may, link), (may, insid), (may, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW DELHI: The BJP has lodged a complaint with...</td>\n",
       "      <td>[new, delhi, bjp, lodg, complaint, elect, comm...</td>\n",
       "      <td>[(lodg, new), (lodg, delhi), (lodg, bjp), (lod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wittenberg Investment Management Inc. lowered ...</td>\n",
       "      <td>[wittenberg, invest, manag, inc, lower, hold, ...</td>\n",
       "      <td>[(inc, wittenberg), (inc, invest), (inc, manag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        full_content  \\\n",
       "0  In closing arguments for Sam Bankman-Fried's c...   \n",
       "1  Perry Carpenter is Chief Evangelist for KnowBe...   \n",
       "2  When you buy through our links, Insider may ea...   \n",
       "3  NEW DELHI: The BJP has lodged a complaint with...   \n",
       "4  Wittenberg Investment Management Inc. lowered ...   \n",
       "\n",
       "                           preprocessed_full_content  \\\n",
       "0  [close, argument, sam, bankmanfri, crimin, tri...   \n",
       "1  [perri, carpent, chief, evangelist, knowb, inc...   \n",
       "2  [buy, link, insid, may, earn, affili, commissi...   \n",
       "3  [new, delhi, bjp, lodg, complaint, elect, comm...   \n",
       "4  [wittenberg, invest, manag, inc, lower, hold, ...   \n",
       "\n",
       "                                target_context_pairs  \n",
       "0  [(bankmanfri, close), (bankmanfri, argument), ...  \n",
       "1  [(evangelist, perri), (evangelist, carpent), (...  \n",
       "2  [(may, buy), (may, link), (may, insid), (may, ...  \n",
       "3  [(lodg, new), (lodg, delhi), (lodg, bjp), (lod...  \n",
       "4  [(inc, wittenberg), (inc, invest), (inc, manag...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data_sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending all the context-target pairs to create our data set for the cbow model\n",
    "model_data = [word for sub_target_context_pairs_list in news_data_sub.target_context_pairs for word in sub_target_context_pairs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decis', 'crimin'),\n",
       " ('decis', 'case'),\n",
       " ('testifi', 'madeth'),\n",
       " ('testifi', 'unusu'),\n",
       " ('testifi', 'decis'),\n",
       " ('testifi', 'crimin'),\n",
       " ('testifi', 'case'),\n",
       " ('testifi', 'potentiallyrisk'),\n",
       " ('crimin', 'unusu'),\n",
       " ('crimin', 'decis')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 351150 (target, context) pairs!\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(model_data)} (target, context) pairs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping our tokens to indices and vice versa  \n",
    "def token_mapping(vocabulary: list) -> tuple:\n",
    "    '''\n",
    "    INPUT:\n",
    "        vocabulary: a list of unique tokens across our corpus, in alphabetical order\n",
    "    OUTPUT:\n",
    "        (word2index, index2word):\n",
    "            word2index: a dictionary with the keys being tokens, and the values being indices\n",
    "            index2word: a dictionary with the keys being indices, and the values being tokens\n",
    "    '''\n",
    "    word2index = dict()\n",
    "    index2word = dict()\n",
    "    for index, token in enumerate(vocabulary):\n",
    "        word2index[token] = index\n",
    "        index2word[index] = token\n",
    "\n",
    "    return (word2index, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index_dict, index2word_dict = token_mapping(vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs = list()\n",
    "\n",
    "for val in model_data:\n",
    "    target = val[0]\n",
    "    context = val[1]\n",
    "    training_pairs.append((word2index_dict[target],word2index_dict[context]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(498, 1022), (498, 353), (498, 4940), (498, 1320), (498, 5824)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, center_word):\n",
    "        embedding = self.embed(center_word)\n",
    "        output = self.linear(embedding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SkipGram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, loss_fn, epochs=100, batch_size=32):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i + batch_size]\n",
    "            center_words, context_words = zip(*batch)\n",
    "            center_words = torch.LongTensor(center_words)\n",
    "            context_words = torch.LongTensor(context_words)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(center_words)\n",
    "            loss = loss_fn(outputs, context_words)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 10.3306\n",
      "Epoch 1, Loss: 9.8582\n",
      "Epoch 2, Loss: 11.7958\n",
      "Epoch 3, Loss: 11.7787\n",
      "Epoch 4, Loss: 11.7625\n",
      "Epoch 5, Loss: 11.6886\n",
      "Epoch 6, Loss: 10.6799\n",
      "Epoch 7, Loss: 9.5930\n",
      "Epoch 8, Loss: 11.3218\n",
      "Epoch 9, Loss: 11.2477\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train(model, training_pairs, optimizer, loss_fn, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6381, 100])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed.weight.data.shape # V x N matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding for 'crimin': [[ 0.6524583  -0.11030392  0.08138298  0.0712603  -0.01790532  0.5110589\n",
      "   0.14788385  0.07459371  0.24618366  0.17944013 -0.03617151  0.4555257\n",
      "  -0.1924063   0.10443083  0.51467943  0.16954361  1.108788    0.09267556\n",
      "  -0.05829034  0.24714412 -0.27283612 -0.21425495 -0.3757515  -0.10037919\n",
      "   0.1660098  -0.17708589  0.49903297  0.09123883 -0.18389769 -0.74992573\n",
      "  -0.01808611  0.23615855  0.12774436 -0.3017926   0.62091327 -0.06487557\n",
      "   0.11615281  0.01948647 -0.20326275 -0.38602903  0.1141483  -0.24586317\n",
      "  -0.29403657  0.15456884 -0.6076997  -0.36643687  0.13225085 -0.3912071\n",
      "   0.42355117 -0.23048699 -1.1307319   0.07886155 -0.29142532 -0.4941296\n",
      "  -0.08560681 -1.1921178  -0.27141258 -0.23502968  0.40122065 -0.15172422\n",
      "  -0.33440393 -0.22674124  0.3290078  -0.2649746  -0.30342585  0.06082836\n",
      "   0.05070811  0.15376125 -0.17219119 -0.4478474   0.1294718  -0.06213101\n",
      "   0.05291691 -0.06071489 -0.11313146  0.187523    0.0077584  -0.05461296\n",
      "   0.19394918  0.1862925  -0.7413885  -0.08739713 -0.0961291  -0.11625725\n",
      "   0.3088574   0.01857572 -0.15478721 -0.20298094  0.05935875 -0.14422804\n",
      "  -0.24573675 -0.01225371  0.00645091  0.4011621  -0.13669732 -0.300078\n",
      "  -0.31943968  0.19470796  0.08659597 -0.36824793]]\n",
      "Embedding shape: (1, 100)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(model, word_to_index, word):\n",
    "    model.eval()\n",
    "    index = word_to_index[word]\n",
    "    embedding = model.embed(torch.LongTensor([index]))\n",
    "    return embedding.detach().numpy()\n",
    "\n",
    "sample_embedding = get_embedding(model, word2index_dict, \"crimin\")\n",
    "print(f\"\\nEmbedding for 'crimin': {sample_embedding}\")\n",
    "print(f\"Embedding shape: {sample_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embedding[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model weights\n",
    "word_vectors = model.embed.weight.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6381, 100])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random sample of word indices from vocabulary\n",
    "random.seed(123)\n",
    "sample_size = len(vocab) #use len(vocab) if you want to use entire vocabulary\n",
    "random_sample = random.sample(range(len(vocab)), sample_size)\n",
    "random_words = [vocab[i] for i in random_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attorney', 'forb', 'braverman', 'would', 'mahmoud']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the word embeddings\n",
    "word_vectors = [get_embedding(model, word2index_dict, w)[0].tolist() for w in random_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6381, 100)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(word_vectors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(x: list,y: list) -> float:\n",
    "    '''\n",
    "    INPUT:\n",
    "        x, y: input word vectors\n",
    "    OUTPUT:\n",
    "        cosine_score: cosine similarity score between x and y\n",
    "    '''\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_word_similarity(word_list: list, embedding_list: list, k: int):\n",
    "    '''\n",
    "    INPUT:\n",
    "        word_list: a list of words for which iteratively word similarity with every word in the vocabulary (except itself) will be calculated\n",
    "        embedding_list: a list where the 'i'th element is the word vector for the word 'i' in word_list\n",
    "        k: int value of the number of most similar words to display for each input word\n",
    "    OUTPUT:\n",
    "        similarity_df: a dataframe with cols 'input_word', 'top_1', 'top_2',...,'top_k', \n",
    "        where 'top_i' will store (output_word, cosine_score)\n",
    "        top_df: storing the a filtered version of similarity_df where rows that have at least one cosine score >=0.75 is retained\n",
    "    '''\n",
    "    similarity_df = pd.DataFrame()\n",
    "    top_df = pd.DataFrame()\n",
    "    c=1\n",
    "    for input_word in tqdm(word_list):\n",
    "        input_word_dict = dict()\n",
    "        for word in filter(lambda x: x !=input_word, word_list):\n",
    "            word_vec_x = word_vectors[word_list.index(input_word)]\n",
    "            word_vec_y = word_vectors[word_list.index(word)]\n",
    "            score = similarity(word_vec_x, word_vec_y)\n",
    "            input_word_dict[word] = score # cosine score of input_word with an output word\n",
    "        \n",
    "        # sort the dictionary by value of scores\n",
    "        input_word_dict = sorted(input_word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        row_df = pd.DataFrame({'input_word':input_word}, index = [c])\n",
    "        row_df = pd.concat([row_df, \n",
    "                            pd.DataFrame({f'top_{i+1}': list(map(lambda x: f'({x[0]},{round(x[1],2)})',input_word_dict))[i]\n",
    "        for i in range(k)}, index = [c])], axis=1)\n",
    "        similarity_df = pd.concat([similarity_df, row_df], axis = 0)\n",
    "\n",
    "        if all([y > 0.6 for x,y in input_word_dict[:k]]):\n",
    "            top_df = pd.concat([top_df, row_df], axis = 0)\n",
    "\n",
    "\n",
    "        c+=1\n",
    "    return similarity_df, top_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6381/6381 [47:06<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "df1, df2 = display_word_similarity(word_list=random_words,\n",
    "                            embedding_list=word_vectors,\n",
    "                            k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_word</th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>fish</td>\n",
       "      <td>(sheep,0.75)</td>\n",
       "      <td>(poultri,0.71)</td>\n",
       "      <td>(cattl,0.63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>cardiovascular</td>\n",
       "      <td>(fibrot,0.67)</td>\n",
       "      <td>(hematolog,0.65)</td>\n",
       "      <td>(immunolog,0.63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>icici</td>\n",
       "      <td>(mahindra,0.65)</td>\n",
       "      <td>(laggard,0.54)</td>\n",
       "      <td>(tcs,0.52)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>mussolini</td>\n",
       "      <td>(dictat,0.68)</td>\n",
       "      <td>(fascist,0.67)</td>\n",
       "      <td>(benito,0.63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>earthquak</td>\n",
       "      <td>(himalayan,0.67)</td>\n",
       "      <td>(seismic,0.63)</td>\n",
       "      <td>(ncs,0.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>acid</td>\n",
       "      <td>(amino,0.65)</td>\n",
       "      <td>(vitamin,0.56)</td>\n",
       "      <td>(antioxid,0.53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>bipolar</td>\n",
       "      <td>(disord,0.55)</td>\n",
       "      <td>(candid,0.54)</td>\n",
       "      <td>(schizophrenia,0.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>inject</td>\n",
       "      <td>(liquid,0.54)</td>\n",
       "      <td>(dosag,0.53)</td>\n",
       "      <td>(semisolid,0.51)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6016</th>\n",
       "      <td>swine</td>\n",
       "      <td>(livestock,0.67)</td>\n",
       "      <td>(cattl,0.63)</td>\n",
       "      <td>(sheep,0.61)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          input_word             top_1             top_2                top_3\n",
       "84              fish      (sheep,0.75)    (poultri,0.71)         (cattl,0.63)\n",
       "839   cardiovascular     (fibrot,0.67)  (hematolog,0.65)     (immunolog,0.63)\n",
       "932            icici   (mahindra,0.65)    (laggard,0.54)           (tcs,0.52)\n",
       "2188       mussolini     (dictat,0.68)    (fascist,0.67)        (benito,0.63)\n",
       "2422       earthquak  (himalayan,0.67)    (seismic,0.63)            (ncs,0.6)\n",
       "2537            acid      (amino,0.65)    (vitamin,0.56)      (antioxid,0.53)\n",
       "4074         bipolar     (disord,0.55)     (candid,0.54)  (schizophrenia,0.5)\n",
       "5219          inject     (liquid,0.54)      (dosag,0.53)     (semisolid,0.51)\n",
       "6016           swine  (livestock,0.67)      (cattl,0.63)         (sheep,0.61)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row 84, 839, 932, 2188, 2422, 2537, 4074, 5219, 6016\n",
    "specific_indices = [84, 839, 932, 2188, 2422, 2537, 4074, 5219, 6016]\n",
    "df2.loc[specific_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_word</th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attorney</td>\n",
       "      <td>(prosecutor,0.55)</td>\n",
       "      <td>(roo,0.48)</td>\n",
       "      <td>(forest,0.46)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forb</td>\n",
       "      <td>(pick,0.38)</td>\n",
       "      <td>(fair,0.31)</td>\n",
       "      <td>(punish,0.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>braverman</td>\n",
       "      <td>(andi,0.34)</td>\n",
       "      <td>(deislam,0.33)</td>\n",
       "      <td>(kwara,0.32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>would</td>\n",
       "      <td>(globe,0.38)</td>\n",
       "      <td>(politicallyconnect,0.37)</td>\n",
       "      <td>(coalit,0.37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mahmoud</td>\n",
       "      <td>(abba,0.5)</td>\n",
       "      <td>(abdullah,0.4)</td>\n",
       "      <td>(pray,0.37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6377</th>\n",
       "      <td>alnasr</td>\n",
       "      <td>(wish,0.5)</td>\n",
       "      <td>(rantisi,0.44)</td>\n",
       "      <td>(chanc,0.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378</th>\n",
       "      <td>hydrat</td>\n",
       "      <td>(humancaus,0.41)</td>\n",
       "      <td>(networkdens,0.34)</td>\n",
       "      <td>(liga,0.33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6379</th>\n",
       "      <td>profession</td>\n",
       "      <td>(model,0.43)</td>\n",
       "      <td>(daili,0.4)</td>\n",
       "      <td>(prior,0.38)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6380</th>\n",
       "      <td>renu</td>\n",
       "      <td>(vig,0.52)</td>\n",
       "      <td>(vicechancellor,0.44)</td>\n",
       "      <td>(düsseldorf,0.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6381</th>\n",
       "      <td>counterfeit</td>\n",
       "      <td>(endoclear,0.39)</td>\n",
       "      <td>(child,0.36)</td>\n",
       "      <td>(warrant,0.36)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6381 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       input_word              top_1                      top_2  \\\n",
       "1        attorney  (prosecutor,0.55)                 (roo,0.48)   \n",
       "2            forb        (pick,0.38)                (fair,0.31)   \n",
       "3       braverman        (andi,0.34)             (deislam,0.33)   \n",
       "4           would       (globe,0.38)  (politicallyconnect,0.37)   \n",
       "5         mahmoud         (abba,0.5)             (abdullah,0.4)   \n",
       "...           ...                ...                        ...   \n",
       "6377       alnasr         (wish,0.5)             (rantisi,0.44)   \n",
       "6378       hydrat   (humancaus,0.41)         (networkdens,0.34)   \n",
       "6379   profession       (model,0.43)                (daili,0.4)   \n",
       "6380         renu         (vig,0.52)      (vicechancellor,0.44)   \n",
       "6381  counterfeit   (endoclear,0.39)               (child,0.36)   \n",
       "\n",
       "                 top_3  \n",
       "1        (forest,0.46)  \n",
       "2         (punish,0.3)  \n",
       "3         (kwara,0.32)  \n",
       "4        (coalit,0.37)  \n",
       "5          (pray,0.37)  \n",
       "...                ...  \n",
       "6377       (chanc,0.4)  \n",
       "6378       (liga,0.33)  \n",
       "6379      (prior,0.38)  \n",
       "6380  (düsseldorf,0.4)  \n",
       "6381    (warrant,0.36)  \n",
       "\n",
       "[6381 rows x 4 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
